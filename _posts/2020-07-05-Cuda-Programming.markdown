---
layout:     post
title:      "Accelerated Computing (I)"
subtitle:   " \"Accelerating Applications with CUDA C/C++\""
date:       2020-07-10 22:00:00
author:     "vince"
header-img: "img/home-bg.jpg"
catalog: true
tags:
    - GPU
    - CUDA
    - C++
---

CUDA provides a coding paradigm that extends languages like C, C++, Python, and Fortran, to be capable of running accelerated, massively parallelized code on the performant parallel processors: NVIDIA GPUs.<br> 
CUDA accelerates applications drastically with little effort, has an ecosystem of highly optimized libraries for DNN, BLAS, graph analytics, FFT and more.
![Image](/img/in-post/200705 CudaProgramming/logo.png)

## Accelerating Applications with CUDA C/C++

**Objectives**
1. Call/Launch GPU kernels
2. Control parallel thread hierarchy using execution configuration
3. Refactor serial loops to execute their iterations in parallel on a GPU
4. Allocate and free memory available to both CPUs and GPUs
5. Handle errors generated by CUDA code
6. Accelerate CPU-only applications

###  Accelerated System
Accelerated systems, also referred to as heterogeneous systems (异构系统), are those composed of both CPUs and GPUs.<br>
Accelerated systems run CPU programs which in turn, launch functions that will benefit from the massive parallelism provided by GPUs.<br>
Using Systems Management Interface cmd to check N-GPU environment:<br>
`!nvidia-smi`<br>

GPU-accelerated vs. CPU-only Applications:<br>
In CPU-only applications data is allocated on CPU and all work is performed on CPU.<br>
In accelerated applications data is allocated with `cudaMallocManaged()` where it can be accessed and worked on by the CPU and automatically migrated to the GPU where parallel work can be done.<br>
Work on the GPU is asynchronous, and CPU can work at the same time. CPU code can sync with the asynchronous GPU work, waiting for it to complete with `cudaDeviceSynchronize()`. data accesses by the CPU will automatically be migrated.<br>
![Image](/img/in-post/200705 CudaProgramming/1.png)


#### Code for the GPU
`.cu` is the file extension for CUDA-accelerated programs.
```cpp
void CPUFunction()
{
  printf("run on the CPU.\n");
}

__global__ void GPUFunction()
{
  printf("run on the GPU.\n");
}

int main()
{
  CPUFunction();

  GPUFunction<<<1, 1>>>();
  cudaDeviceSynchronize();
}
```
`__global__ void GPUFunction()`
  - **The `__global__` keyword indicates that the following function will run on the GPU**, and can be invoked **globally**, which in this context means either by the CPU, or, by the GPU.
  - Often, code executed on the CPU is referred to as **host** code, and code running on the GPU is referred to as **device** code.

`GPUFunction<<<1, 1>>>();`
  - Typically, when calling a function to run on the GPU, we call this function a **kernel**, which is **launched**.
  - When launching a kernel, we must provide an **execution configuration**, which is done by using the `<<< ... >>>` syntax just prior to passing the kernel any expected arguments.
  - At a high level, execution configuration allows programmers to specify the **thread hierarchy** for a kernel launch, which **defines the number of thread groupings (called blocks), as well as how many threads to execute in each block**. Execution configuration will be explored at great length later in the lab, but for the time being, notice the kernel is launching with `1` block of threads (the first execution configuration argument) which contains `1` thread (the second configuration argument).

`cudaDeviceSynchronize();`
  - Unlike much C/C++ code, launching kernels is **asynchronous** (异步): the CPU code will continue to execute **without waiting for the kernel launch to complete**.
  - A call to `cudaDeviceSynchronize`, a function provided by the CUDA runtime, will cause the host (CPU) code to wait until the device (GPU) code completes, and then resume execution on the CPU.


#### Compiling and Running Accelerated CUDA Code
The CUDA platform ships with the NVIDIA CUDA Compiler nvcc, which can compile CUDA accelerated applications, both the host, and the device code they contain.<br>
`nvcc` will be very familiar to experienced `gcc` users. Compiling a `some-CUDA.cu` file:<br>
`nvcc -arch=sm_70 -o out some-CUDA.cu -run`<br>
  - The `o` flag is used to specify the output file for the compiled program.
  - The `arch` flag indicates for which **architecture** the files must be compiled. For the present case `sm_70` will serve to compile specifically for the Volta GPUs this lab is running on, but for those interested in a deeper dive, please refer to the docs about the [`arch` flag](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#options-for-steering-gpu-code-generation), [virtual architecture features](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list) and [GPU features](http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list).
  - As a matter of convenience, providing the `run` flag will execute the successfully compiled binary.

### CUDA Thread Hierarchy
GPUs do work in parallel, and each GPU work is done in a thread. Many threads run in parallel.
![Image](/img/in-post/200705 CudaProgramming/2.png)
As mentioned above, GPU function is called kernel and Kernels are launched with an execution configuration.

#### Launching Parallel Kernels
Execution configurations: ```<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>```<br>
**The kernel code is executed by every thread in every thread block configured when the kernel is launched**.


### CUDA-Provided Thread Hierarchy Variables
Just as threads are grouped into thread blocks, blocks are grouped into a grid, which is the highest entity in the CUDA thread hierarchy. In summary, CUDA kernels are executed in a grid of 1 or more blocks, with each block containing the same number of 1 or more threads.

#### Accelerating For Loops
For loops in CPU-only applications are ripe for acceleration: rather than run each iteration of the loop serially, each iteration of the loop can be run in parallel in its own thread.<br>
Consider the following for loop:<br>
```cpp
int N = 2<<20;
for (int i = 0; i < N; ++i)
{
  printf("%d\n", i);
}
```
In order to parallelize this loop, 2 steps must be taken:

- A kernel must be written to do the work of a **single iteration of the loop**.
- Because the kernel will be agnostic of other running kernels, the execution configuration must be such that the kernel executes the correct number of times, for example, the number of times the loop would have iterated.<br>

```cpp
__global__ void loop()
{
  printf("This is iteration number %d\n", threadIdx.x);
}
```

### Coordinating Parallel Threads (协调并行线程)
Assuming data is in a 0 indexed vector. Each thread must be mapped to work on an element in the vector.<br>
Using the formula: $ threadIdx.x + blockIdx.x \times blockDim.x $ will map each thread to one element in the vector.<br>
![Image](/img/in-post/200705 CudaProgramming/2.png)

#### Using Block Dimensions for More Parallelization
